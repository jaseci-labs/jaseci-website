[{"frontmatter":{"title":"Harnessing the Power of Generative AI: Insights and Use Cases from Jason Mars","description":"meta description","date":"2023-04-04T00:00:00.000Z","image":"/images/posts/post-1.png","categories":["AI + Business"],"authors":["Jaseci"],"tags":["AI + Business"],"draft":false},"content":"\n_This article is based on a talk by Jason Mars at the Michigan Technology Leaders Summit, presented by SIM Detroit, on the topic of Artificial Intelligence: Actual Use Cases._\n\n## What criteria do you use to prioritize AI projects in your portfolio and how often do you ensure alignment with broader business objectives?\n\nIn the last year, we’ve partnered with companies to bring two generative AI solutions to production, observing key drivers across multiple sectors. These drivers can be categorized into two main themes.\n\nFirstly, companies see AI as an opportunity to scale productivity, market output, and business efficiency. On the other hand, they perceive AI as a risk, navigating a new competitive landscape where survival depends on capitalizing on AI opportunities. This is particularly evident in the financial sector, where companies recognize the need to compete in a more efficient market.\n\nThe launch of ChatGPT marked a significant shift in the market, with VC investment in AI skyrocketing from $2 billion to $14 billion within six months. Companies realize that to survive, they need to compete in a higher efficiency landscape. This realization has led to a surge in market and internal capability analyses to identify the safest starting points for AI implementation.\n\n## Financial & Business Use Cases of Generative AI\n\nA notable trend is the democratization of AI, which has become a mandate rather than an option. Smaller, specialized models have emerged, offering cost-effective alternatives to large foundation models like GPT-4.\n\nFor example, we’ve worked with PocketNest, a Michigan-based company, to build a conversational AI focused on financial advice. By using smaller open-source models, we’ve significantly reduced costs while maintaining competitive quality.\n\nAnother innovative use case is TOBU, a product that allows users to attach memories to pictures through a conversational AI. This AI interacts with users about their experiences and the context of their photos, creating a personalized memory assistant.\n\n## What are some of the most significant challenges you’ve encountered in implementing these projects and what have you learned from it?\n\nCost remains a significant challenge when scaling AI solutions. While development costs might be manageable during the initial phase, launching a product to thousands of users can become prohibitively expensive. This has driven a deeper investigation into using challenger models like Mistral and Llama to balance cost and performance effectively.\n\nExpertise within partner companies also plays a crucial role in the successful deployment of AI. Our consulting approach involves delivering IP and production-ready AI engines while navigating challenges such as bounding AI use cases to prevent liability and ensuring that AI solutions remain within desired parameters.\n\n## How do you assess state and federal regulations, security and the privacy?\n\nAs we consume more information through screens, the realm of AI expands. It’s essential to control this growth thoughtfully, ensuring AI remains a tool for good. The current phase of AI development involves creating tools to harness the full potential of these models, akin to refining raw ore.\n\nTools like Lang Chain, funded by Sequoia, exemplify the innovation in this space. These tools help solve bigger problems by enabling seamless interaction with large language models. Personalization remains a key focus, making AI more accessible and tailored to individual needs.\n\nIn conclusion, the landscape of AI is evolving rapidly, with companies seeking to balance opportunities and risks. Through careful analysis, cost-effective solutions, and innovative use cases, businesses can harness the power of generative AI to drive efficiency and personalization in unprecedented ways.\n","slug":"post1"},{"frontmatter":{"title":"Tech firm that delivered AI for US Bank and Barclays organizes a Jaseci Hackathon","description":"meta description","date":"2023-04-04T00:00:00.000Z","image":"/images/posts/post-10-1.jpeg","categories":["Developers"],"authors":["V75"],"tags":["Developers"],"draft":false},"content":"\nOne of Guyana’s prominent technology firms which specializes in conversational AI and enterprise systems development, V75 Inc organized a two-day boot camp to get their technical team ramped up on a relatively new but powerful technology stack called Jaseci (find out more [here](https://jaseci.org)). Jaseci is an open source AI ecosystem bringing with it an open computational model, technology stack and methodology designed to enable developers to rapidly build robust products with sophisticated AI capabilities, at scale. V75 Inc has been in the pro-serve technology business since 2014 through its predecessor-in-interest Version75 Solutions which was later incorporated as V75 Inc in 2019. In 2018 the company partnered with conversational AI firm, Clinc Inc based in Ann Arbor, MI and entered the conversational AI engineering space, which at its peak, saw twenty-five certified conversational AI engineers from V75 that helped build over 90% of Clinc’s deliveries to clients such as OCBC, US Bank, Barclays and others.\n\n![Launch](/images/posts/post-10-2.png)\n\nAs a progressive technology start-up in a developing country, V75 Inc particularly appreciates the importance of wisely investing the relatively limited human and financial resources available, especially operating in a environment following the pandemic. V75’s leadership immediately recognized the immense value that the Jaseci open source ecosystem could bring, not just for their planned pro-serve deliveries but for their aspirations to enter the product space. The design of Jaseci’s technology stack would provide enough abstraction and developer ease-of-use to wield AI engines and handle complex infrastructure challenges with deployment without the requirement of deep domain knowledge.\n\nFrom April 26-27, V75’s leadership organized a two-day Jaseci Hackathon to serve as a bootcamp to ramp up select members of V75’s technical staff on the new technology. A total of eighteen members joined remotely and in-person for this event. The event was led by V75’s founder, Eldon Marks, who underwent a personal ramp-up journey on the technology stack before organizing the knowledge-sharing exercise in the form of the hackathon. During the hackathon proceedings, Jaseci Labs cofounder and the creator of Jaseci, Prof. Jason Mars joined in to introduce Jaseci to the team and answer questions about the stack and its capabilities. The two-day hackathon took the team through the set-up of Jaseci, the basics of its glue language called JAC as well as the development paradigm of the stack. By the second day, the team was building out their planned conversational flows and leveraging the Universal Sentence Encoder in the creation of a pre-trained, chatbot that was surprisingly capable at handling question-answer type exchanges. The team also observed that Jaseci provided a very robust micro-services based infrastructure upon which APIs could be built, with or without AI capabilities.\n","slug":"post10"},{"frontmatter":{"title":"Setting Up Jaseci On Apple M1 Macs (ARM Processors)","description":"meta description","date":"2023-04-04T00:00:00.000Z","image":"/images/posts/post-11-1.jpg","categories":["Developers"],"authors":["Jaseci"],"tags":["Developers"],"draft":false},"content":"\nCore Jaseci and its built-in libraries run great on an M1 mac, with Rosetta enabled. However, using packages such as use_qa can result in errors such as Illegal instruction: 4, followed by python crashing and a VERY LONG list of errors, most of which you cannot make sense of…\n\n![Code 1](/images/posts/post-11-2.png)\n\n## The Short (and Sweet) Way… Use Remote Actions\n\nThis is also my favorite method to use…\n\nTo load use_qa and other jaseci modules, you can use the remote modules set up by our Sifus. To do this, basically, replace `actions load module jaseci_kit.use_qa` with `actions load remote https://use-qa.jaseci.org`\n\n![Code 2](/images/posts/post-11-3.png)\n\nAnd, that’s all folks…\n\nOr is it? we are hardcore programmers and don’t feel satisfied using the “short & sweet way” of doing things, do we? So let’s look at…\n\n## The Other Way\n\nThis is the fun way, that’ll require reading a host of documentation… which we’ll try to avoid… but then end up reading it through thoroughly after hours of avoiding reading the documentation…\n\nSo here’s that perfect setup we need to get this all up and running.\n\n### Update Jaseci & Packages\n\nEnsure you’re running the correct version of Python needed for the version of Jaseci you’re running, then update Jaseci & Jaseci Kit by running.\n\n- `pip3 install jaseci --upgrade`\n- `pip3 install jaseci-kit --upgrade`\n\n### Tensorflow\n\nTurns out, that Tensorflow does not work too nicely with the new ARM processors that apple is using and requires special versions to run on the new M1 processors… So you’d need to read this article from Apple: [https://developer.apple.com/metal/tensorflow-plugin/](https://developer.apple.com/metal/tensorflow-plugin/)\n\n_Quick Note_: Ensure the TensorFlow version you’re installing matches the requirements of the Jaseci version you’re currently running, else it won’t work.\n\n### Test Tensorflow\n\nEnsure you test TensorFlow using the Jupyther example in the video above (start at the 5-minute mark if you installed Tensorflow from the article) to ensure that TensorFlow is running smoothly.\n","slug":"post11"},{"frontmatter":{"title":"Inter-American Development Bank funded Jaseci AI apprenticeship program – July 1, 2022","description":"meta description","date":"2023-04-04T00:00:00.000Z","image":"/images/posts/post-12.png","categories":["Developers"],"authors":["Jaseci"],"tags":["Developers"],"draft":false},"content":"\nSelect participants from the Jaseci AI / Spark program, which saw nearly two hundred Guyanese youth upskilled in leadership and AI tracks, gained the opportunity to further their foundational AI training through the Jaseci AI apprenticeship program. This program was funded by the Inter-American Development Bank (IDB) Lab under project GY-T1162 – Developing Guyana’s ICT Sector. The IDB is a development-focused bank that provides multilateral financing and expertise for sustainable economic and institutional development in Latin America and the Caribbean. The project’s executing agency, Nexus Hub Inc., in collaboration with Jaseci Labs LLC worked with local technology firm, V75 Inc. to facilitate the industrial attachment of ten (10) graduates from the Jaseci AI / Spark program comprising individuals from high school, the University of Guyana, Computer Science Department and local industry.\n\nThe ten chosen apprentices will undergo a focused, three month industrial attachment with V75 Inc. as they work on building various AI products to deepen their understanding of the Jaseci Open Source ecosystem as well as the field of AI. The program will be executed remotely, allowing the participants to work in their own time but according to scheduled milestone deliveries. Each participant will receive a monthly stipend as they progress through the program as well as the opportunity to be drafted into one of the AI-focused teams within Jaseci Labs or V75 Inc. The program also presents the apprentices with a tech entrepreneurship pathway with the opportunity to continue to develop their AI products towards commercialization under the guidance of Jaseci Labs and the support of V75 Inc.\n","slug":"post12"},{"frontmatter":{"title":"A Whale of a Tale: The Size-Matters Misconception For Generative AI","description":"meta description","date":"2023-04-04T00:00:00.000Z","image":"/images/posts/post-2.jpg","categories":["Developers"],"authors":["Forbes"],"tags":["Developers"],"draft":false},"content":"\nIn this new age of generative AI, everyone has made a major assumption for which a pressing question has emerged. You can see this manifesting in certain nerdy corners of social media.\n\nThis screenshot of Reddit and Twitter posts shows the rising curiosity about the effectiveness of small, open-source models versus their large, proprietary counterparts. Indeed, in this noisy market, there is widespread confusion and curiosity surrounding this topic. The narrative that “bigger is inherently better” is about to be challenged.\n\n![_Is bigger necessarily better when it comes to AI models? Maybe not.](/images/posts/post-2.1.png)\n_Is bigger necessarily better when it comes to AI models? Maybe not._\n\nWell, along with my amazing colleagues at the University of Michigan and Jaseci Labs, we’ve delved into this very question in a rigorous and scholarly way. We’ve produced the first academic paper that addresses the debate head-on, to be presented in the prestigious ISPASS 2024 proceedings.\n\nOur findings are not just surprising; they are a call to rethink what we know about the size of AI models we should be relying on in production and commercial use cases, and the efficiency we can achieve.\n\n## Two Major AI Contenders\n\nSo, let’s talk about the two contenders, the Large Language Models (GPT4 and friends) vs the Small Language Models. Open AI has published GPT4 models that are at least 540 gigabytes, while the small and open-source models we study in the paper are around three gigabytes. That’s around 200 times smaller.\n\nTo illustrate the comparison, imagine GPT-4 as the blue whale, the largest animal on the planet, weighing up to about 200 tons. Now, contrast this with a housefly, a creature so small it’s easy to overlook, weighing in at a mere 12 milligrams.\n\nThese houseflies would be our models like LLaMA-7b quantized, Mistral-7b quantized, and Starling-LM-7b quantized — smaller, open-source alternatives poised to challenge the notion that bigger always means better. This comparison represents the difference in scale between the models we study in the paper.\n\nThe core discovery in the paper is simple: the belief that one must wield a GPT-4-sized model to achieve significant results is a myth.\n\n## Our Approach\n\nOur research was conducted with open and quantized models and gpt4 itself. Our investigation was centered around a case study with the commercial Myca.ai product, a productivity tool enhanced by AI to deliver personalized pep talks based on your productivity. The results, as detailed in our paper, are nothing short of shocking even to us.\n\nWe asked three simple questions. Can end users tell a quality loss in response when using the housefly models? How much faster are the AI responses with the smaller open models? And how much cheaper is it?\n\n## On Quality\n\n![Response quality of GPT-4 and SLMs as rated by human reviewers.](/images/posts/post-2.2.png)\n_Response quality of GPT-4 and SLMs as rated by human reviewers._\n\nWhen participants were subjected to a blind test comparing the output of large proprietary models against that of smaller, open-source models, the results were revelatory. Like the famed Pepsi/Coke taste tests, users were hard-pressed to discern which model produced the output. Indeed, much of the time, OpenAI’s GPT4 was not selected or scored very poorly. GPT4 was selected as the better output only around half the time than an SLM. For many (perhaps most) practical product use cases, SLMs do not only as well as sometimes even better than generalized proprietary LLMs. This result underscored the competency of smaller models in delivering quality content indistinguishable from their larger counterparts.\n\n## On Speed\n\nFurther analysis revealed that these smaller models are up to 10 times faster than GPT-4 on our own machines in an AWS cluster and offer greater reliability. The latency of response was consistent all day long.\n\nAnd Myca.ai didn’t suffer from the outages that OpenAI has become known for. Given that our housefly models are not tethered to the operational integrity of any single provider, they remain unaffected by these outages that can impact any of the larger, proprietary models.\n\n## On Cost\n\nPerhaps most compelling is the cost advantage. Our research indicates that deploying small, open models can be anywhere from five to 23 times cheaper than relying on a model like GPT-4. This range represents a worst-case to best-case scenario, highlighting the substantial financial benefits that come with adopting smaller models.\n\n## The Groundbreaking Insight\n\nWhen you opt for smaller, more accessible models, you not only gain control but also empower yourself with the ability to tailor the technology to your needs. Businesses, for example, can take these open-source models and adapt them, even going as far as training them in-house, without the prohibitive costs associated with larger models.\n\nOur findings invite a paradigm shift in how we approach the development and deployment of AI models. The evidence is clear: smaller, open-source models not only stand toe-to-toe with their gargantuan counterparts in terms of intelligence and capability but also offer critical advantages.\n\nIndeed, Jaseci Labs is now helping businesses tailor their own small models for game-changing product use cases, leading to what may be a major description of the OpenAIs and Anthropics of the world.\n\nWe encourage you to delve into the peer-reviewed analysis presented in our paper. Let the truth behind this rigorous analysis guide your decisions as you navigate the future of AI, and consider how embracing smaller models could not only enhance your technological endeavors but also democratize access to this groundbreaking field.\n\nThis article was originally posted on Forbes.com, click the link below to read the complete article.\n\n[Read the full article on Forbes](https://www.forbes.com/sites/forbesbooksauthors/2024/03/21/a-whale-of-a-tale-the-size-matters-misconception-for-generative-ai/?sh=40121c8c581a)\n","slug":"post2"},{"frontmatter":{"title":"Can Tiny Titans Take On The Giants in Generative AI? How Small Language Models Are Matching Up to Large Language Models.","description":"meta description","date":"2023-04-04T00:00:00.000Z","image":"/images/posts/post-3.jpg","categories":["Developers"],"authors":["Forbes"],"tags":["Developers"],"draft":false},"content":"\nWith the rapid evolution of large language models (LLMs), the world has begun to rely on the APIs of managed AI models such as OpenAI’s GPT-4. Its cutting-edge capabilities and developer-friendly interface have gained a large fan base not only in academia but also in the industry-wide as well. As every rose has its thorn, there are some downsides to those proprietary APIs despite all the benefits they bring to the table. Performance reliability, uptime predictability, and cost are some of them. At the same time, a flurry of open-source small language models (SLMs) has been made available for commercial use.\nSo, here we face the ultimate question,\n\n## Is GPT-4 really worth it?\n\nor can those open-source SLMs replace proprietary LLMs?\nBut before we dive into this question, another problem comes to play.\n\n### How are we going to evaluate proprietary LLM vs open-source SLM?\n\nIn the paper, the authors introduce a new toolset, SLaM to evaluate the performance of SLM vs LLM across a wide range of metrics.\n\nSLaM is a system that facilitates the process of hosting and evaluating language models on the AWS cloud. It works by downloading models from the Hugging Face repository and setting them up for experiments. To evaluate the quality of these models, SLaM uses both human and automated methods. For human evaluation, SLaM provides a user interface (UI) where humans can rate model responses without knowing which model generated which response. For automated evaluation, SLaM uses a Similarity Scorer and GPT-4-based scoring to assess response quality based on similarity metrics. In addition to this, SLaM performs automatic performance evaluations by measuring query latency over time.\n\nWhen investigating this feature in a real-world product, the authors have considered a ‘PEP-TALK’ feature in a task management and productivity application called ‘Myca’. When users log into the app at the start of their day, it gives them a positive message tailored to their previous day’s achievements, plans, and overall progress toward their goals. The ‘PEP-TALK’ feature combines the user’s past activities and plans into a prompt, fed into a language model, which generates a motivational response displayed to the user.\n\nSo, this [paper](https://arxiv.org/pdf/2312.14972) discusses four questions related to moving from proprietary LLMs to self-hosted SLMs.\n\n1. Is the quality of SLMs good enough for users?\n2. How well can AI-assisted tooling automate the process of identifying SLM alternatives?\n3. What are the latency implications of self-hosted SLMs in a utility-based cloud environment?\n4. What are the cost tradeoffs of open-source SLMs compared to proprietary LLM APIs?\n\nIn this article we are going to talk about the quality of the content generated by SLMs.\nThis box distribution depicts the performances of 9 SLMs all together with their 29 quantized models which were reviewed by humans along with a proprietary LLM.\n\nThis shows that some smaller language models (SLMs) can create responses just as good as, or even better than, the larger models made by OpenAI. Especially, the smaller, optimized versions of these models, called quantized models, are doing really well, which is great for using them in real-world applications because they take up less space. Most of the SLMs tested produce responses that are almost as good as the best models, indicating that the quality of their responses is quite similar to that of OpenAI’s models. However, a few specific models, namely the orca2:7b (both base and optimized versions) and stablelm-zephyr:3b-q3, didn’t perform as well and had noticeably poorer response quality compared to others.\n\n### So, SLMs can indeed produce good content just as good as proprietary LLMs\n\nbut the debate on whether they can replace LLMs continues. And we can’t come to a final decision without the answers to other three questions.\n","slug":"post3"},{"frontmatter":{"title":"What’s In Store For The Next Generation Of AI? The Jaseci Perspective","description":"meta description","date":"2023-04-04T00:00:00.000Z","image":"/images/posts/post-4.jpg","categories":["AI + Business"],"authors":["Forbes"],"tags":["AI + Business"],"draft":false},"content":"\n_This article is based on a talk by Jason Mars at the Michigan Technology Leaders Summit, presented by SIM Detroit, on the topic of Artificial Intelligence: Actual Use Cases._\n\n## What criteria do you use to prioritize AI projects in your portfolio and how often do you ensure alignment with broader business objectives?\n\nIn the last year, we’ve partnered with companies to bring two generative AI solutions to production, observing key drivers across multiple sectors. These drivers can be categorized into two main themes.\n\nFirstly, companies see AI as an opportunity to scale productivity, market output, and business efficiency. On the other hand, they perceive AI as a risk, navigating a new competitive landscape where survival depends on capitalizing on AI opportunities. This is particularly evident in the financial sector, where companies recognize the need to compete in a more efficient market.\n\nThe launch of ChatGPT marked a significant shift in the market, with VC investment in AI skyrocketing from $2 billion to $14 billion within six months. Companies realize that to survive, they need to compete in a higher efficiency landscape. This realization has led to a surge in market and internal capability analyses to identify the safest starting points for AI implementation.\n\n## Financial & Business Use Cases of Generative AI\n\nA notable trend is the democratization of AI, which has become a mandate rather than an option. Smaller, specialized models have emerged, offering cost-effective alternatives to large foundation models like GPT-4.\n\nFor example, we’ve worked with PocketNest, a Michigan-based company, to build a conversational AI focused on financial advice. By using smaller open-source models, we’ve significantly reduced costs while maintaining competitive quality.\n\nAnother innovative use case is TOBU, a product that allows users to attach memories to pictures through a conversational AI. This AI interacts with users about their experiences and the context of their photos, creating a personalized memory assistant.\n\n## What are some of the most significant challenges you’ve encountered in implementing these projects and what have you learned from it?\n\nCost remains a significant challenge when scaling AI solutions. While development costs might be manageable during the initial phase, launching a product to thousands of users can become prohibitively expensive. This has driven a deeper investigation into using challenger models like Mistral and Llama to balance cost and performance effectively.\n\nExpertise within partner companies also plays a crucial role in the successful deployment of AI. Our consulting approach involves delivering IP and production-ready AI engines while navigating challenges such as bounding AI use cases to prevent liability and ensuring that AI solutions remain within desired parameters.\n\n## How do you assess state and federal regulations, security and the privacy?\n\nAs we consume more information through screens, the realm of AI expands. It’s essential to control this growth thoughtfully, ensuring AI remains a tool for good. The current phase of AI development involves creating tools to harness the full potential of these models, akin to refining raw ore.\n\nTools like Lang Chain, funded by Sequoia, exemplify the innovation in this space. These tools help solve bigger problems by enabling seamless interaction with large language models. Personalization remains a key focus, making AI more accessible and tailored to individual needs.\n\nIn conclusion, the landscape of AI is evolving rapidly, with companies seeking to balance opportunities and risks. Through careful analysis, cost-effective solutions, and innovative use cases, businesses can harness the power of generative AI to drive efficiency and personalization in unprecedented ways.\n","slug":"post4"},{"frontmatter":{"title":"What Is Conversational AI? ZeroShot Bot CEO Jason Mars Explains","description":"meta description","date":"2023-04-04T00:00:00.000Z","image":"/images/posts/post-5.png","categories":["Developers"],"authors":["Nvidia"],"tags":["Developers"],"draft":false},"content":"\n_This article is based on a talk by Jason Mars at the Michigan Technology Leaders Summit, presented by SIM Detroit, on the topic of Artificial Intelligence: Actual Use Cases._\n\n## What criteria do you use to prioritize AI projects in your portfolio and how often do you ensure alignment with broader business objectives?\n\nIn the last year, we’ve partnered with companies to bring two generative AI solutions to production, observing key drivers across multiple sectors. These drivers can be categorized into two main themes.\n\nFirstly, companies see AI as an opportunity to scale productivity, market output, and business efficiency. On the other hand, they perceive AI as a risk, navigating a new competitive landscape where survival depends on capitalizing on AI opportunities. This is particularly evident in the financial sector, where companies recognize the need to compete in a more efficient market.\n\nThe launch of ChatGPT marked a significant shift in the market, with VC investment in AI skyrocketing from $2 billion to $14 billion within six months. Companies realize that to survive, they need to compete in a higher efficiency landscape. This realization has led to a surge in market and internal capability analyses to identify the safest starting points for AI implementation.\n\n## Financial & Business Use Cases of Generative AI\n\nA notable trend is the democratization of AI, which has become a mandate rather than an option. Smaller, specialized models have emerged, offering cost-effective alternatives to large foundation models like GPT-4.\n\nFor example, we’ve worked with PocketNest, a Michigan-based company, to build a conversational AI focused on financial advice. By using smaller open-source models, we’ve significantly reduced costs while maintaining competitive quality.\n\nAnother innovative use case is TOBU, a product that allows users to attach memories to pictures through a conversational AI. This AI interacts with users about their experiences and the context of their photos, creating a personalized memory assistant.\n\n## What are some of the most significant challenges you’ve encountered in implementing these projects and what have you learned from it?\n\nCost remains a significant challenge when scaling AI solutions. While development costs might be manageable during the initial phase, launching a product to thousands of users can become prohibitively expensive. This has driven a deeper investigation into using challenger models like Mistral and Llama to balance cost and performance effectively.\n\nExpertise within partner companies also plays a crucial role in the successful deployment of AI. Our consulting approach involves delivering IP and production-ready AI engines while navigating challenges such as bounding AI use cases to prevent liability and ensuring that AI solutions remain within desired parameters.\n\n## How do you assess state and federal regulations, security and the privacy?\n\nAs we consume more information through screens, the realm of AI expands. It’s essential to control this growth thoughtfully, ensuring AI remains a tool for good. The current phase of AI development involves creating tools to harness the full potential of these models, akin to refining raw ore.\n\nTools like Lang Chain, funded by Sequoia, exemplify the innovation in this space. These tools help solve bigger problems by enabling seamless interaction with large language models. Personalization remains a key focus, making AI more accessible and tailored to individual needs.\n\nIn conclusion, the landscape of AI is evolving rapidly, with companies seeking to balance opportunities and risks. Through careful analysis, cost-effective solutions, and innovative use cases, businesses can harness the power of generative AI to drive efficiency and personalization in unprecedented ways.\n","slug":"post5"},{"frontmatter":{"title":"The Future Of Artificial Intelligence In Guyana","description":"meta description","date":"2023-04-04T00:00:00.000Z","image":"/images/posts/post-6.png","categories":["Developers"],"authors":["V75"],"tags":["Developers"],"draft":false},"content":"\n_This based on a talk by Jason Mars at the Michigan Technology Leaders Summit, presented by SIM Detroit, on the topic of Artificial Intelligence: Actual Use Cases._\n\n## What criteria do you use to prioritize AI projects in your portfolio and how often do you ensure alignment with broader business objectives?\n\nIn the last year, we’ve partnered with companies to bring two generative AI solutions to production, observing key drivers across multiple sectors. These drivers can be categorized into two main themes.\n\nFirstly, companies see AI as an opportunity to scale productivity, market output, and business efficiency. On the other hand, they perceive AI as a risk, navigating a new competitive landscape where survival depends on capitalizing on AI opportunities. This is particularly evident in the financial sector, where companies recognize the need to compete in a more efficient market.\n\nThe launch of ChatGPT marked a significant shift in the market, with VC investment in AI skyrocketing from $2 billion to $14 billion within six months. Companies realize that to survive, they need to compete in a higher efficiency landscape. This realization has led to a surge in market and internal capability analyses to identify the safest starting points for AI implementation.\n\n## Financial & Business Use Cases of Generative AI\n\nA notable trend is the democratization of AI, which has become a mandate rather than an option. Smaller, specialized models have emerged, offering cost-effective alternatives to large foundation models like GPT-4.\n\nFor example, we’ve worked with PocketNest, a Michigan-based company, to build a conversational AI focused on financial advice. By using smaller open-source models, we’ve significantly reduced costs while maintaining competitive quality.\n\nAnother innovative use case is TOBU, a product that allows users to attach memories to pictures through a conversational AI. This AI interacts with users about their experiences and the context of their photos, creating a personalized memory assistant.\n\n## What are some of the most significant challenges you’ve encountered in implementing these projects and what have you learned from it?\n\nCost remains a significant challenge when scaling AI solutions. While development costs might be manageable during the initial phase, launching a product to thousands of users can become prohibitively expensive. This has driven a deeper investigation into using challenger models like Mistral and Llama to balance cost and performance effectively.\n\nExpertise within partner companies also plays a crucial role in the successful deployment of AI. Our consulting approach involves delivering IP and production-ready AI engines while navigating challenges such as bounding AI use cases to prevent liability and ensuring that AI solutions remain within desired parameters.\n\n## How do you assess state and federal regulations, security and the privacy?\n\nAs we consume more information through screens, the realm of AI expands. It’s essential to control this growth thoughtfully, ensuring AI remains a tool for good. The current phase of AI development involves creating tools to harness the full potential of these models, akin to refining raw ore.\n\nTools like Lang Chain, funded by Sequoia, exemplify the innovation in this space. These tools help solve bigger problems by enabling seamless interaction with large language models. Personalization remains a key focus, making AI more accessible and tailored to individual needs.\n\nIn conclusion, the landscape of AI is evolving rapidly, with companies seeking to balance opportunities and risks. Through careful analysis, cost-effective solutions, and innovative use cases, businesses can harness the power of generative AI to drive efficiency and personalization in unprecedented ways.\n","slug":"post6"},{"frontmatter":{"title":"V75’s Jaseci Apprenticeship Program","description":"meta description","date":"2023-04-04T00:00:00.000Z","image":"/images/posts/post-7.png","categories":["Developers"],"authors":["V75"],"tags":["Developers"],"draft":false},"content":"\n_This article is based on a talk by Jason Mars at the Michigan Technology Leaders Summit, presented by SIM Detroit, on the topic of Artificial Intelligence: Actual Use Cases._\n\n## What criteria do you use to prioritize AI projects in your portfolio and how often do you ensure alignment with broader business objectives?\n\nIn the last year, we’ve partnered with companies to bring two generative AI solutions to production, observing key drivers across multiple sectors. These drivers can be categorized into two main themes.\n\nFirstly, companies see AI as an opportunity to scale productivity, market output, and business efficiency. On the other hand, they perceive AI as a risk, navigating a new competitive landscape where survival depends on capitalizing on AI opportunities. This is particularly evident in the financial sector, where companies recognize the need to compete in a more efficient market.\n\nThe launch of ChatGPT marked a significant shift in the market, with VC investment in AI skyrocketing from $2 billion to $14 billion within six months. Companies realize that to survive, they need to compete in a higher efficiency landscape. This realization has led to a surge in market and internal capability analyses to identify the safest starting points for AI implementation.\n\n## Financial & Business Use Cases of Generative AI\n\nA notable trend is the democratization of AI, which has become a mandate rather than an option. Smaller, specialized models have emerged, offering cost-effective alternatives to large foundation models like GPT-4.\n\nFor example, we’ve worked with PocketNest, a Michigan-based company, to build a conversational AI focused on financial advice. By using smaller open-source models, we’ve significantly reduced costs while maintaining competitive quality.\n\nAnother innovative use case is TOBU, a product that allows users to attach memories to pictures through a conversational AI. This AI interacts with users about their experiences and the context of their photos, creating a personalized memory assistant.\n\n## What are some of the most significant challenges you’ve encountered in implementing these projects and what have you learned from it?\n\nCost remains a significant challenge when scaling AI solutions. While development costs might be manageable during the initial phase, launching a product to thousands of users can become prohibitively expensive. This has driven a deeper investigation into using challenger models like Mistral and Llama to balance cost and performance effectively.\n\nExpertise within partner companies also plays a crucial role in the successful deployment of AI. Our consulting approach involves delivering IP and production-ready AI engines while navigating challenges such as bounding AI use cases to prevent liability and ensuring that AI solutions remain within desired parameters.\n\n## How do you assess state and federal regulations, security and the privacy?\n\nAs we consume more information through screens, the realm of AI expands. It’s essential to control this growth thoughtfully, ensuring AI remains a tool for good. The current phase of AI development involves creating tools to harness the full potential of these models, akin to refining raw ore.\n\nTools like Lang Chain, funded by Sequoia, exemplify the innovation in this space. These tools help solve bigger problems by enabling seamless interaction with large language models. Personalization remains a key focus, making AI more accessible and tailored to individual needs.\n\nIn conclusion, the landscape of AI is evolving rapidly, with companies seeking to balance opportunities and risks. Through careful analysis, cost-effective solutions, and innovative use cases, businesses can harness the power of generative AI to drive efficiency and personalization in unprecedented ways.\n","slug":"post7"},{"frontmatter":{"title":"A Jaseci Outreach Story","description":"meta description","date":"2023-04-04T00:00:00.000Z","image":"/images/posts/post-8.png","categories":["Developers"],"authors":["Jaseci"],"tags":["Developers"],"draft":false},"content":"\n_This article is based on a talk by Jason Mars at the Michigan Technology Leaders Summit, presented by SIM Detroit, on the topic of Artificial Intelligence: Actual Use Cases._\n\n## What criteria do you use to prioritize AI projects in your portfolio and how often do you ensure alignment with broader business objectives?\n\nIn the last year, we’ve partnered with companies to bring two generative AI solutions to production, observing key drivers across multiple sectors. These drivers can be categorized into two main themes.\n\nFirstly, companies see AI as an opportunity to scale productivity, market output, and business efficiency. On the other hand, they perceive AI as a risk, navigating a new competitive landscape where survival depends on capitalizing on AI opportunities. This is particularly evident in the financial sector, where companies recognize the need to compete in a more efficient market.\n\nThe launch of ChatGPT marked a significant shift in the market, with VC investment in AI skyrocketing from $2 billion to $14 billion within six months. Companies realize that to survive, they need to compete in a higher efficiency landscape. This realization has led to a surge in market and internal capability analyses to identify the safest starting points for AI implementation.\n\n## Financial & Business Use Cases of Generative AI\n\nA notable trend is the democratization of AI, which has become a mandate rather than an option. Smaller, specialized models have emerged, offering cost-effective alternatives to large foundation models like GPT-4.\n\nFor example, we’ve worked with PocketNest, a Michigan-based company, to build a conversational AI focused on financial advice. By using smaller open-source models, we’ve significantly reduced costs while maintaining competitive quality.\n\nAnother innovative use case is TOBU, a product that allows users to attach memories to pictures through a conversational AI. This AI interacts with users about their experiences and the context of their photos, creating a personalized memory assistant.\n\n## What are some of the most significant challenges you’ve encountered in implementing these projects and what have you learned from it?\n\nCost remains a significant challenge when scaling AI solutions. While development costs might be manageable during the initial phase, launching a product to thousands of users can become prohibitively expensive. This has driven a deeper investigation into using challenger models like Mistral and Llama to balance cost and performance effectively.\n\nExpertise within partner companies also plays a crucial role in the successful deployment of AI. Our consulting approach involves delivering IP and production-ready AI engines while navigating challenges such as bounding AI use cases to prevent liability and ensuring that AI solutions remain within desired parameters.\n\n## How do you assess state and federal regulations, security and the privacy?\n\nAs we consume more information through screens, the realm of AI expands. It’s essential to control this growth thoughtfully, ensuring AI remains a tool for good. The current phase of AI development involves creating tools to harness the full potential of these models, akin to refining raw ore.\n\nTools like Lang Chain, funded by Sequoia, exemplify the innovation in this space. These tools help solve bigger problems by enabling seamless interaction with large language models. Personalization remains a key focus, making AI more accessible and tailored to individual needs.\n\nIn conclusion, the landscape of AI is evolving rapidly, with companies seeking to balance opportunities and risks. Through careful analysis, cost-effective solutions, and innovative use cases, businesses can harness the power of generative AI to drive efficiency and personalization in unprecedented ways.\n","slug":"post8"},{"frontmatter":{"title":"Jaseci Labs collaborates with Government of Guyana on mass AI upskilling programme","description":"meta description","date":"2023-04-04T00:00:00.000Z","image":"/images/posts/post-9-1.jpeg","categories":["Developers"],"authors":["Jaseci"],"tags":["Developers"],"draft":false},"content":"\nThe Jaseci Spark AI Programme which aims to introduce students to Artificial Intelligence (AI) technology was launched today at the National Centre for Educational Resource Development (NCERD).\n\nThe programme is a product of the collaboration between the Ministry of Education and the LEAD Mindset, JASECI Labs and BCS Technology.\n\n![Launch](/images/posts/post-9-2.png)\n\n_Jason Mars explained that the programme will run for eight weeks with two tracks, one that focuses on developing the leadership and innovative mindset while on the other, participants will be exposed to the AI technical track. He said that the AI track which will teach the students how to code and use the same instruments used by the biggest companies in the world to build AI products and services. He said that along the AI track, students will be required to build a real AI product that could be launched to the world._\n\nStudents from more than 10 secondary schools across the country would be participating in the programme. Read more about this program on the Department of Public Information (Guyana) website.\n\n[Read more here](https://dpi.gov.gy/students-to-be-introduced-to-ai-technology-as-spark-programme-launches/)\n","slug":"post9"}]